### Data Modeling Case Study Prep 

Overview: This scenario-based interview will focus on your `product sense and data modeling skills`. You’ll be assessed on
data literacy,\
data warehousing,\
ETL implementation,\
collaboration skills, 
and your product sense. 

You will be asked to **design some metrics** and
create an **entity-relationship diagram**, and will create an
ETL flow using the data model you designed. You will be
evaluated not only on the strength of your solution, but
also your approach and level of autonomy in working
through the problem. You will be collaborating with the
interviewer through a Google Doc. You will be asked to
write SQL, but the code does not need to have perfect syntax.

What we’re looking for:\
● Problem Solving: We want to get a sense of how you think through ambiguous problems!
We’re not necessarily looking for the perfect, most efficient solution immediately, but want
to see how you iterate to get there.\
● Communication: Can you explain your thought process and how you arrived at a
solution? Can you receive feedback throughout the interview and course correct? We are
looking for effective communication throughout the duration of the interview, especially if
your solution isn’t working the way you intended it to.

Tips:\
● Interactive Interview: Be prepared for a highly interactive session. Asking clarifying
questions and seeking a comprehensive understanding of the problem is crucial.\
● Articulate Thought Process: If faced with difficulties, explain your solution approach and
reasoning. This transparency enables interviewers to provide valuable hints and insights.\
● Focus on the Question: The interview is designed to guide you step by step towards the
final solution. Please do not jump ahead to solutioning in the earlier sections of the
interview, where we would still be discussing the scenario.\
● Scalability: We encourage you to think about how your solution would scale with
increasing data volume and complexity.


### Resources
<details>
<summary> <a href="https://www.tryexponent.com/courses/data-engineering/data-modeling-interviews/data-modeling-intro">1. Exponent</a>
</summary>

- Identifying business requirements and technical constraints
- Designing a dimensional model for a data warehouse with appropriate fact and dimension tables.
- Design trade-offs: How you weigh different design options and articulate their pros and cons.
- Performance considerations: Your understanding of how to optimize the model for query performance and scalability.
- Creating an ER diagram of your solution, typically including 3-5 tables
- Adapting your data model to new requirements or feedback.


Key areas of assessment include:

1. Defining appropriate grain for fact tables
2. Identifying relevant dimensions and facts
3. Handling slowly changing dimensions
4. Considering query patterns and performance implications
5. Addressing data volume and scalability concerns
6. Articulating design choices and trade-offs

</details>

### Preparation 
<details>
<summary> Different modeling approaches</summary>
Understand trade-offs: Study the pros and cons of different modeling approaches, such as star schemas vs. snowflake schemas, or normalized vs. denormalized designs.

It’s about understanding the bigger picture:
1. Diagnosis: If DAU or MAU drops, how would you diagnose it? Which metrics would you check? What questions would you ask?
2. Product Strategy: How would you improve user retention? What data would help drive that decision?

</details>


<details>
<summary> Dimensional Modeling</summary>
Ensure a thorough understanding of dimensional modeling, including fact tables, dimension tables, and slowly changing dimensions


### Fact data Model 
- Transaction
- Periodic Snapshot Fact Table 
- Accumulating Snapshot
- Bridge Tables (Factless Fact Tables)

</details>


### Experience
<details>
<summary> Online  </summary>

1. Fixed question - wearable device track, query writing, user avg, how many days login, logout date, BI idea, build data modal - how will keys interact? (Manish Kumar Youtube)
2. You're a PM at a food delivery app where conversion rates have declined over the past week. How would you investigate the causes? (Conversion: From users browsing to placing orders.)
3. [On DoorDash, there are missing item and wrong item issues for deliveries. How would you analyze each of them?](https://www.tryexponent.com/questions/4862/doordash-order-issue-analysis)
4. Growth, scaling challenges for setting up a new DoorDash campaign (Glassdoor)

</details>

## Questions 
<details>
<summary> 1. How to model CDC ?</summary>
</details>

<details>
<summary> 1. How to model SDC ?</summary>
</details>

## Examples 
<details>
<summary> 1. Real Time Walmart System Design </summary>

Initially business explained by interviewer - (They said transactional but expectation was analytical)

1. Asked for DAU = 20 million 
2. 86400 sec in a day - rounded to 10^5  (we know this)
3. 20* 10^6 / 10^5 = 200 users per sec 
4. they are writing 5 things so 200*5 = 1000 writes/sec in DB
5. Request will be 2.5 times of this = 2500 

Transactional System 
- Latency
- Consistency

Q- He asked, Why are you doing all this?
A- To asses how large the data will be, which DB should be used, latency, schema evolve, sql or no sql should be used? 
He asked for tables - wanted to go for analytical system - dimensional modelling 

Q- I asked, Which portion you want to make ? Order 
A- I said, okay Order and user 

Order Table 
- user_id (FK)
- order_id (PK)
- order_date
- order_amount
- quantity
- discount

User Table 
- user_id (PK)
- login_date 
- logout_date

Q: He asked Where are Shipment details, cart, whishlist, inventory? 
Q: Asked One to many or many to many mapping

Shipping Table (Acummulating Snapshot) - to analyse where clogging is happening in supply chain 
                                       - All dates as we know multiple steps in a shipping process
- order_id
- order_date 
- dispatch_date
- shipment_date 
- out_for_delivery_date 
- delivered_date

Cart Table 
- user_id
- product_id
- cart_added_date

Product Table 
- product_id (PK)
- seller_id

Seller Profile Table 
- seller_id
- seller


Q: If we have to search in NoSQL, user will search a query in amazon search bar, how will you do? 
A: Elastic Search - text search easier in it 
Product name - if user make a typo, we have to show all relevant products 
Let's say user type datawarehouse then we have to show him all books containing that keyword 

Q: How will you implement this? Write query for this
A: 

Q: How will user journey happen for this search and how will you return to user? He wanted to test microservices 
A: User_login -> authentication -> success -> search button /api/v1/search/ -> payload(keyword) -> will keep pools and choose one to make elastic connection -> pass payload to query -> response of the page url -> parse on the user page (encoding json, protobuff, Atlassian recently changed)

LLD 
Q: Write code to implement this 
app.post("/api/v1/search")
 Search.search()

Tried to use Design Pattern in this 

ABC is abstract base class in python 
whosoever inherit this class will have to implement these functions 

class abs_methos(ABC):
    def __init__(self):
        self.abc = None

    def search():
        pass

    def response():
        pass

class search(abs_method):
    def __init__(self):
        self.config = config 
    
    def search("text_serach"):
        # make connection
        
        return result 

</details>

<details>
<summary> 2. Wearable devices for delivery partners </summary>
 
 - To track activity metrics like steps taken, distance traveled, and active time during deliveries. 
 - The goal is to improve `Dasher efficiency`, `optimize delivery routes`, and enhance overall health and safety.
 - Design a data model to store and analyze this wearable device data, define key metrics for success, and outline how the data will be processed. 

### Answer
1. Entities
    - Dasher
        - dasher_id: VARCHAR(50), PRIMARY KEY, Unique identifier for each Dasher
        - name: VARCHAR(100), NOT NULL, Dasher’s full name
        - start_date: DATE, NOT NULL, Date joined DoorDash
    - Wearable
        - wearable_id
        - dasher_id as FK 
    - Activity_Log
        - log_id
        - wearable_id as FK
        - timestamp
        - lat
        - long
        - steps
        - distance_travelled
        - heart_rate
    - Order
        - order_id
        - dasher_id as FK
        - customer_id as FK
        - restaurant_id as FK
        - timestamps
        - rating
    - Customer
    - Restaurant
    - External_Factors
    - I used a separate Activity_Log table for scalability since wearable data is high-frequency
2. Relationships:
    - Dasher   1:N Wearable (one Dasher can have multiple devices over time).
    - Wearable 1:N Activity_Log (one device logs many entries).
    - Dasher   1:N Order (one Dasher handles many orders).
    - Order    N:1 Customer 
    - Order    N:1 Restaurant (many orders per customer/restaurant).
3. primary keys (PK) and foreign keys (FK) on connectors

Invite Questions: 
1. Does this align with DoorDash’s needs? 
2. Are there other entities or relationships you’d suggest adding?

Mention
1. Scalability: Given DoorDash’s volume, I’d partition Activity_Log by date and index timestamp and wearable_id for fast joins. This handles millions of daily logs without performance hits.
2. Data Quality: I’d build ETL checks to flag invalid lat/long or missing timestamps before loading into these tables, ensuring reliable analysis.
3. Extensibility: This model can expand to include new wearable metrics (e.g., calories burned) by adding columns to Activity_Log, or new external data (e.g., real-time traffic APIs) via External_Factors.
4. Collaboration: I’d validate this with product teams to confirm fields like rating meet reporting needs, and with infra teams to align on warehouse constraints.

Adapt to Feedback: 
1. If the interviewer challenges a design (e.g., “Why not denormalize Activity_Log into Order?”), explain your reasoning (e.g., `“Normalization helps with wearable data volume and updates”`) and offer alternatives (`“We could denormalize for specific dashboards if query speed is critical”`).

### Generate Metrics and SQL 
Step 1: Define Objectives and Assumptions
- My assumptions here are:
    1. Timestamps are in a consistent format (e.g., UTC) and mostly non-null.
    2. Rating is on a 1-5 scale.
    3. Wearable data in Activity_Log is high-frequency but can be aggregated


1. Dasher Efficiency - Average Delivery Time per Dasher, and correlate with wearable activity data like distance_travelled.
    ```sql
    SELECT dasher_id, 
        AVG(EXTRACT(EPOCH FROM (delivery_time - pickup_time))/60) AS avg_del_time_minutes  -- convert to minute 
    FROM orders 
    WHERE order_time >= CURRENT_DATE - INTERVAL '30 days' --current date 
    AND pickup_time IS NOT NULL -- remember this
    AND delivery_time IS NOT NULL -- edge case 
    GROUP BY dasher_id
    ORDER BY avg_del_time_minutes
    ```
2. Customer Satisfaction - Delivery Time and Rating Correlation
   
   - Thresholds: I’ll bucket delivery times (<20 mins, 20-30, >30) to see rating trends rather than a binary ‘related’ label, addressing aggregation needs.

   - Regional Trends: JOIN with Customer to group by approximate region using lat/long rounded to a grid (e.g., 0.1 degree), addressing the cross-question on geography.
   
   - Scalability: Indexing on order_time and customer_id for joins; consider materializing regional grids if frequently queried.

   - Buckets delivery times and groups by approximate region (lat/long rounded for simplicity), showing average ratings per bucket and area. It reveals if faster deliveries consistently yield higher ratings and if geography plays a role (e.g., urban vs. rural delays)

        ```sql 
        SELECT 
        CASE 
            WHEN EXTRACT(EPOCH FROM (o.delivery_time - o.pickup_time))/60 < 20 THEN '<20 mins'
            WHEN EXTRACT(EPOCH FROM (o.delivery_time - o.pickup_time))/60 < 30 THEN '20-30 mins'
            ELSE '>30 mins'
        END AS delivery_time_bucket,
        ROUND(c.lat, 1) || ',' || ROUND(c.long, 1) AS approx_region,
        COUNT(*) AS total_orders,
        ROUND(AVG(o.rating), 2) AS avg_rating
        FROM orders o
        JOIN customer c ON o.customer_id = c.customer_id
        WHERE o.order_time >= CURRENT_DATE - INTERVAL '30 days'
            AND o.pickup_time IS NOT NULL
            AND o.delivery_time IS NOT NULL
            AND o.rating IS NOT NULL
        GROUP BY 
            CASE 
                WHEN EXTRACT(EPOCH FROM (o.delivery_time - o.pickup_time))/60 < 20 THEN '<20 mins'
                WHEN EXTRACT(EPOCH FROM (o.delivery_time - o.pickup_time))/60 < 30 THEN '20-30 mins'
                ELSE '>30 mins'
            END,
            ROUND(c.lat, 1) || ',' || ROUND(c.long, 1)
        ORDER BY delivery_time_bucket, avg_rating DESC;
        ```
4. Health and Safety: Identifying Overexertion via Heart Rate

    - Objective: Flag Dashers with high average heart_rate (>120 bpm) during shifts, check delivery count correlation.

        ```sql
        SELECT 
        d.dasher_id,
        DATE(al.timestamp) AS activity_date,
        ROUND(AVG(al.heart_rate), 2) AS avg_heart_rate_bpm,
        COUNT(DISTINCT o.order_id) AS deliveries_completed
        FROM dasher d
        JOIN wearable w ON d.dasher_id = w.dasher_id
        JOIN activity_log al ON w.wearable_id = al.wearable_id
        LEFT JOIN orders o ON d.dasher_id = o.dasher_id
            AND DATE(o.order_time) = DATE(al.timestamp) -- is okay to join with timestamp?
        WHERE al.timestamp >= CURRENT_DATE - INTERVAL '7 days' -- is this needed - should be order date
            AND al.heart_rate IS NOT NULL
        GROUP BY d.dasher_id, DATE(al.timestamp)
        HAVING AVG(al.heart_rate) > 120
        ORDER BY avg_heart_rate_bpm DESC;
        ```
5. Route Optimization Check: Comparing Actual vs. Straight-Line Distance
    
    Objective: Check if Dashers take efficient routes by comparing distance_travelled to straight-line distance
     - Sum distance_travelled from Activity_Log between pickup_time and delivery_time
     - compute straight-line distance using Haversine formula approximation with lat/long
     - approximate straight-line as a baseline (real routes aren’t straight due to roads)
     - Scalability: Location data is voluminous; limit to recent orders and optimize joins
        ```sql
        SELECT 
            o.order_id,
            o.dasher_id,
            ROUND(SUM(al.distance_travelled), 2) AS actual_distance_km,
            ROUND(
                6371 * ACOS(
                    COS(RADIANS(r.lat)) * COS(RADIANS(c.lat)) * 
                    COS(RADIANS(c.long) - RADIANS(r.long)) + 
                    SIN(RADIANS(r.lat)) * SIN(RADIANS(c.lat))
                ), 2
            ) AS straight_line_distance_km,
            CASE 
                WHEN SUM(al.distance_travelled) > 1.5 * (6371 * ACOS(
                    COS(RADIANS(r.lat)) * COS(RADIANS(c.lat)) * 
                    COS(RADIANS(c.long) - RADIANS(r.long)) + 
                    SIN(RADIANS(r.lat)) * SIN(RADIANS(c.lat))
                )) THEN 'Inefficient'
                ELSE 'Efficient'
            END AS route_efficiency
            FROM orders o
            JOIN restaurant r ON o.restaurant_id = r.restaurant_id
            JOIN customer c ON o.customer_id = c.customer_id
            JOIN activity_log al ON o.dasher_id = al.wearable_id::text -- Adjust linkage as needed
                AND al.timestamp BETWEEN o.pickup_time AND o.delivery_time
            WHERE o.order_time >= CURRENT_DATE - INTERVAL '7 days'
                AND o.pickup_time IS NOT NULL
                AND o.delivery_time IS NOT NULL
            GROUP BY o.order_id, o.dasher_id, r.lat, r.long, c.lat, c.long
            ORDER BY actual_distance_km DESC
            LIMIT 100;
        ```

</details>
