# System Design - 60 minutes

Overview: This interview covers services architecture
and systems design. You’ll be assessed on your ability to
design a high-level data E2E system while diving deep
into areas like data ingestion, data processing, scalability,
and the data persistence layer. You’ll be evaluated on
your ability to come up with efficient solutions to
open-ended problems by applying your knowledge of
systems theory and product development. This interview
will be conducted using an online whiteboard tool. Our
preferred platform for a virtual whiteboard is HackerRank,
however if there is a tool with similar functionality that you are more comfortable with then please
come prepared with it.

What they looking for:\
● Structure: Ensure you take a systematic approach to building your solution and that you
articulate what you are trying to do and why you are trying to do it.\
● Comprehensiveness: Ensure your approach covers all aspects of the requirements and
tackles any potential edge cases.\
● Feasibility: Ensure your solution is practical and could realistically be implemented.\
● Scalability: Ensure your solution has the capacity to scale as we increase users or
broaden the problem requirements.

Tips:\
● Clarifying Questions: Before jumping in to designing your solution, ensure that you
understand all aspects of the given requirements.\
● Trade Offs: Understand the various tradeoffs of the components in your system and
explain them to your interviewer.\
● Articulate Thought Process: If faced with difficulties, explain your solution approach and
reasoning. This transparency enables interviewers to provide valuable hints and insights.



## Experiences

<details>
<summary> Online </summary>

- You have to track user metrics - how will you develop solution for it - real time vs batch discussion 
Project discussion - data modelling - data collection - transformation - where we will write them? what metrics on real time what batch ? what frequency? - dashboard refresh - frequency 
streaming focussed than batch(Manish Kumar Youtube)

(Blind)
- For system design interview. Question was something like hosting a service then backend db design for services and analytics design for data getting logged.
- Business round was more like business use case specific if there is drop in sales. How will you figure out? What data sources to look for ? Then what will be steps.
- Managerial round was on day to day activities. What’s the architecture? How do you handle conflicts? What were initiatives? What mistakes you made? What did you learn from those?

</details>


## Preparation 

<details>
    <summary>  Resources  </summary>

1. [System Design for Data]([https://www.youtube.com/watch?v=OWeQ_gCNe4k) 
2. System design Interview by Alex - Book
3. Design Data Intensive Application - Book
4. Youtube Videos - Arpit Bhayani, Gaurav Sen 
5. [Medium Article - Streamimg Data Pipeline](https://medium.com/@seancoyne/data-engineering-practice-system-design-question-and-solution-streaming-ad32562ba954)
</details>


<details>
    <summary>  High Level Componemts </summary>


Let's say you have to Design an e-commerce platfrom related to data side

- Clarify what you are trying to make 
    - What does e-commerce means ? Nykaa, Amazon
    - Are you asking to build transactional or analytical system?
    - Who will be the end consumer?
    - What is the scale of data, what is anticipated scale in upcoming 6 months or a year?
    - Existing systems or services which I should be aware of ?
    - What is the type of data? - Structured, unstructured, semi-structured?
    - Most important feature which I should start with ?

<h4> Back of envelope estimations </h4>

- Scale 
    - Power of 2 
        - 1 Kb 
        - 1 Mb 
        - 1 GB
        - 1 TB
        - 1 PB

- Latency - Microservices 
    - sending data over networks - compression(Json/Protobuff) - encoding (DDIA book)
    - Compression 
    - Multi region copy latency will matter

- Availability 
    - Pipeline criticality - no downtime - 99%
    - Pipeline Uptime
    - Tier 1, Tier 2, Tier 3 - Exception, ROllback implementation 

- Calculate storage requirements 
    - Total Data volume on a daily basis if you can tell
    - no of total users 
    - DAU or MAU 
    - Always round off the number to nearest 100
    - Daily 200 GB for 1 year = 200 * 400 = 8 * 10^4 = 80000 GB = 80 TB Data 
    - How much time you want to keep the data ? Purge? cold system? 
    - no. of seconds in a day ?

<h4> High Level Design - Draw.io </h4>

- Test Cases (CICD)
    
- Input 
    - Data Format
    - Frequency 
    - Schema Evolution (API data specially)
    - PII data or normal data (Governance/Encryption)

- Business Use Case
    - Metrics to track - promotional activity - inventory ? shelf? 
    - How freq will you track ? Batch or real time? 
    - SLA (how much time will you take - 4 hours- rollback)

- Output
    - Target System 
    - Data Retention 
    - Refresh Frequency 
    - Historical Data  

**Summary**
1. Real Time Analytics 
2. Historical Analysis 
3. Scalability (Data + Processing)
4. Flexible data model

<h4> Building a solution </h4>

Incoming Data Sources 
1. API 
2. DB (SQL/ No SQL)
3. Files

- There can be a system that can pick from API and put into KAFKA or can also put directly from API to KAFKA in real time. 
- 6 hour sync in DB -> S3(Raw Layer) -> How will you pull data into S3? what will be Partitioning strategy? 
- Processing Layer -> (Medallion Architecture)
    - Metrics 1 - Near Real time (FLINK/ Spark Streaming) -> will go to Kafka -> UI (superset) or Dashboard
    - Metric 2 - incremental data (hourly or daily folder) after 6 hour sync -> Snowflake DWH (transient or permanent table / view materialised or normal) or go back to No SQL 
    - Business Usecase 
    - Data Modelling is done one time - Fact and dimension writing - distinct values in dimension - snapshot 
 
1. Why Kafka ?
2. Why SQL, No SQL?
3. Ingestion Layer - Pull Data 
4. Scheduling - Cron/ Airflow 
5. CI/CD (Gitlab/ Code coverage/ Test Cases/ Roll Back)
6. Exception Handling (on whole system, Trade off, Fault Tolerance, roll back )


### Drill Down 
1. API 
- Microservices 
- Event Driven Architecture 
- Pull/ Push Mechanisam 
- Authentication (JWT, SAML)
- Design Pattern -> LLD
- LLD (object , class interaction) how will you pull data from API or Files?
- Async Programming (very infreq, multi processing, multi threading)

2. Database (Most Focus)
- SQL, No SQL
- DB Internals 
- Volume Challenges (if volume is increasing day by day where to keep data DB or DWH)
- Optimization - Query and DB 
- Indexing/ Sharding/ Caching/ Materialized view - which column and what indexing
- ACID 
- CAP Theorem
- Constraints/ Normalization/ Denormalization
- Leader Follower Architecture 
- Connection Pull 

3. File Type 
- Parquet/ORC/CSV - which you have used why? what optimizations?
- structured vs unstructured( how will you process, schema evolution)
- Hudi, Iceberg, Delta Table 

4. S3 (80 TB per year - where should we keep? Business - how much do they need?)
- Cost Analysis 
    - Amazon - starting orders in cache - earlier year they keep in different layer 
    - Logs - Purge because volume is too much - 6 months - machine generated 
- DataLake vs Delta Lake 
- Data at Rest Encryption 
- Partitioning (File and DB)

5. Kafka 
- Backpressure 
- offset management ( where it is stored, how, how can we bring it back)
- Broker/ Producer/ Consumer 
- Kafka Connect 
- Topic and its management(why, when replication)
- Auto Commit and Linger Time 
- Exactly once record process( Failure overcome, so that it is dropped, how kafka manages this? there should be no duplication, how will you resolve this?)
- Failure Overcome 
- Replication 

6. Spark Streaming 
If you will you use Kafka , you need streaming, either you use Microservice Architecture or Flink 
- Flink vs Spark Streaming 
    - Late Data Arrival
    - Sliding window 
    - Checkpointing
    - commits
    - stateful vs stateless
    - event time - one semantics at the time of generation - when data arrived or producer system generated data and why, fault tolerance
    - Fault Tolerance  Question: You have Kafka with retention of 6 hour for logs - Failure of Spark Streaming, retention is over- now how will you ensure logs are there? will you increase retention? - linked in post suggested 3 days - but kafka cluster will be full
    - Performance optimization 
    
7. Processing layer (Not much focus here)
- Dimensional Modelling 
- Lakehouse Architecture 


8. Spark 
- on which platform will you run this ?
- data transformation
- all questions


9. Medallion Architecture
- GOLD Layer 
    - PII data (RBAC Permission) row level and column level 
    - How will you put encryption - UDF in spark - Encryption logic - how to decrypt 
    - Unity Catalog( Databricks)
    

Dimension Modelling 
- ER diagram Practice 
- Fact and Dimension Table 
- Ecommerce (user, sales, inventory)
- Ride sharing App 
- Finance Domain (credit card details) 
- Health Domain 

10. Scheduling/ Orchestration 
- Airflow 
- Internal Working 
- Type of executor 
- DAG/ TASK
- operators/ Sensor
- Custom operator 
- Xcom
- Backfill 
- Pools
- Automation/ Microservices

11. Docker & Kubernetes 


Not Required
- Login management 
- CDN 
- Tokenisation 
- Maps 
- Ride Sharing on Data Side not on Application Side 

</details>


## Questions 

<details>
<summary> 1. How would you make sure API calls with rate limits ? </summary>

</details>

<details>
<summary>2. Large volume of data in S3 which cannot be pulled directly into system - how will you proecess it ? </summary>

</details>


<details>
<summary> 3. Imagine you have millions of IoT devices, emitting sensor data every second. Your goal is to collect this data and process it in real time to detect anomalies, and store both raw and processed data in a scalable data store. How would you design an end to end solution? </summary>

Questions that can be asked [5min - 10 min]

1. Main goal anomaly detection or other analytics needs (predictive maintenance, user behaviour analyis?)
2. Is data structured, semi-structured or unstructured?
3. Data comes in at constant rate or are there spikes in data ? 
4. Need to implement data encryption at rest or in transit?
5. Are there any specific compliance or regulatory requirements for storing sensor data, such as GDPR, HIPPA ?
6. How many devices do you expect initially, how quickly will that number grow? 

</details> <!-- Question 3. -->

<details>
<summary> 4. Let's say you encounter a Spark pipeline performing poorly, how do you go about identifying the issues and optimizing?</summary>

- Step 1: Understand the Symptoms
    Start by identifying the exact issue:
    1. Is the job slow overall or just certain stages?
    2. Is it failing or just taking too long?
    3. Is it CPU-bound, memory-bound, or I/O-bound?

    You can use:
    1. Spark UI (local or in your cluster manager like YARN/EMR/Databricks)
    2. Logs (driver and executor logs)
    3. Metrics dashboards (e.g., Ganglia, CloudWatch, Prometheus)

- Step 2: Profile the Spark Job
    Inspect the Spark UI:
    1. Stages & Tasks: Look for skewed stages, long-running tasks, or failed ones.
    2. Shuffle & Spill: Excessive shuffle and disk spill are red flags.
    3. Job DAG: Check if it's too complex or has unnecessary stages.
    4. Task Distribution: Are all cores utilized? Are some executors idle?

- Step 3: Review the Code and Transformations
    Look for:
    1. Wide transformations like groupBy, join, distinct, repartition — these trigger shuffles.
    2. UDFs — especially Python UDFs in PySpark — which are black-boxes to Spark.
    3. Collect or broadcast misuse.
    4. Unnecessary caching or missing persistence.

- Step 4: Check Data Issues
    1. Data Skew: One or a few keys have too much data (e.g., one country has 90% of rows). Fix via:
    - Salting the key
    - Aggregating before joining
    2. Small files: Can lead to too many tasks. Coalesce or compact beforehand.
    3. Data size: Check partition sizes — aim for 100–200MB per partition.

- Step 5: Tune Spark Configurations
    1. Memory Usage:
    - By default split, execution (for computation) and storage (for caching data)
    - Increase executor memory or storage fraction `(spark.executor.memory, spark.memory.storageFraction)`
    - Also, check spark.executor.memory to set the memory per executor—make sure it fits within your cluster’s node capacity.
    2. Parallelism:
        - Set `spark.sql.shuffle.partitions` appropriately (not default 200 for big data) based on your data size and cluster resources.
        - Tune `spark.default.parallelism`
        - 2-4 tasks per CPU core
        - How many cores are available in your cluster?
    3. Executor and Driver Settings: 
        - Set spark.executor.cores and spark.executor.instances to control how many executors run and how many cores each uses.
        For example, if a node has 16 cores, you might set 5 cores per executor to leave room for OS and other processes.
        - The driver (spark.driver.memory) also needs enough memory, especially for collecting results or running heavy operations.
    3. Garbage Collection: Long GC pauses can kill performance.
    4. Broadcast joins:
    If one side of a join is small (<10MB), enable broadcast joins
    Manually broadcast if Spark doesn’t auto-detect

Step 6: Infrastructure Considerations
Are your executors under-provisioned?
Is auto-scaling working effectively?
Are disk or network IO bottlenecks showing up in metrics?

- Step 7: Iterate & Test
    - After each change, re-run and compare:
        1. Job duration
        2. Shuffle size
        3. Stage/task run times
        4. Executor utilization

</details> <!-- Question 4. -->

<details>
<summary> 5. How do you manage the complexity of a pipeline with tons of transformations? </summary>
</details> <!-- Question 5. -->

<details>
<summary> 6. How do you go about testing this pipeline? </summary>
</details> <!-- Question 6. -->


<details>
<summary> 7. Design shopping cart for website </summary>

1. What is the user count ? 500 K 
2. Active user count ? 100K
    - system needs to be scalable 
3. Traffic spikes in the year or month? 
4. Stateless or stateful (state is maintained on refresh) - cart should be stateful 
5. Region (US, Europe, APAC) for data governance 
6. On prem, on cloud 

### Design 
 
- Load balancer for spikes 
- AutoScaling for compute 

- caching mechanism 
    1. Redis caching, a database with large cache, super performant 
        - Cons manage an extra component and cost
- VPC router - will route to the right region 

Data Pipeline for 
1. Data Source 
2. Batch or Live 
3. Frequency ? 
4. Data Size 
   - 450 GB /month 
5. Already have an system in place? you need to move to cloud from spark
6. Data customers? 
    - BI users
    - DS (ML model)
    - Operational User 

Design 
1. Landing Area - S3 - all data comes here
2. Spark based processing system in AWS? 
3. Store in S3 again 
4. Snowflake - serverless for SQL part 
5. Notebooks for ML people 
6. Orchestration - Airflow 

</details>  <!-- Question 7. -->

<details>
<summary> 8. Design ETL pipeline to ingest data from multiple external APIs 
Handle Schema Evolution </summary>

Airflow @daily vs @once triggered 
Slow sql to optimize
failure handling, retry logic , partitioning 
how will you handle large file in GBs
asked to find top 10 user by event frequency - constraint was optimized for memory
what if streaming data type change mid way
generator functions 

System Design 
Real time Data Pipeline for click stream events 
Ensure fault toleranc 
where deduplication logic 
store 1 billion records 
z ordering 

Behaviour 
- You take full ownership of failing project 
- What do you if deadline is missed because of your code 

</details>