# System Design - 60 minutes

Overview: This interview covers services architecture
and systems design. You’ll be assessed on your ability to
design a high-level data E2E system while diving deep
into areas like data ingestion, data processing, scalability,
and the data persistence layer. You’ll be evaluated on
your ability to come up with efficient solutions to
open-ended problems by applying your knowledge of
systems theory and product development. This interview
will be conducted using an online whiteboard tool. Our
preferred platform for a virtual whiteboard is HackerRank,
however if there is a tool with similar functionality that you are more comfortable with then please
come prepared with it.

What they looking for:\
● Structure: Ensure you take a systematic approach to building your solution and that you
articulate what you are trying to do and why you are trying to do it.\
● Comprehensiveness: Ensure your approach covers all aspects of the requirements and
tackles any potential edge cases.\
● Feasibility: Ensure your solution is practical and could realistically be implemented.\
● Scalability: Ensure your solution has the capacity to scale as we increase users or
broaden the problem requirements.

Tips:\
● Clarifying Questions: Before jumping in to designing your solution, ensure that you
understand all aspects of the given requirements.\
● Trade Offs: Understand the various tradeoffs of the components in your system and
explain them to your interviewer.\
● Articulate Thought Process: If faced with difficulties, explain your solution approach and
reasoning. This transparency enables interviewers to provide valuable hints and insights.



## Experiences

<details>
<summary> Online  </summary>

- You have to track user metrics - how will you develop solution for it - real time vs batch discussion 
Project discussion - data modelling - data collection - transformation - dashboard refresh - frequency - 
streaming focussed (Manish Kumar Youtube)


</details>



## Preparation 

<details>
    <summary>  Resources  </summary>

1. [System Design for Data]([https://www.youtube.com/watch?v=OWeQ_gCNe4k) 
2. System design Interview by Alex - Book
3. Design Data Intensive Application - Book
4. Youtube Videos - Arpit Bhayani, Gaurav Sen 
</details>


<details>
    <summary>  High Level Componemts </summary>


Let's say you have to Design an e-commerce platfrom related to data side

- Clarify what you are trying to make 
    - What does e-commerce means ? Nykaa, Amazon
    - Are you asking to build transactional or analytical system?
    - Who will be the end consumer 
    - What is the scale of data, what is anticipated scale in upcoming 6 months or a year?
    - Existing systems or services which I should be aware of ?
    - most important feature which I should start with ?

<h4> Back of envelope estimations </h4>

- Scale 
    - Power of 2 
        - 1 Kb 
        - 1 Mb 
        - 1 GB
        - 1 TB
        - 1 PB

- Latency - Microservices 
    - sending data over networks - compression(Json/Protobuff) - encoding (DDIA book)
    - Compression 
    - Multi region copy latency will matter

- Availability 
    - Pipeline criticality - no downtime - 99%
    - Pipeline Uptime
    - Tier 1, Tier 2, Tier 3 - Exception, ROllback implementation 

- Calculate storage requirements 
    - Total Data volume on a daily basis if you can tell
    - no of total users 
    - DAU or MAU 
    - Always round off the number to nearest 100
    - Daily 200 GB for 1 year = 200 * 400 = 8 * 10^4 = 80000 GB = 80 TB Data 
    - How much time you want to keep the data ? Purge? cold system? 
    - no. of seconds in a day ?

<h4> High Level Design - Draw.io </h4>

- Test Cases (CICD)
    
- Input 
    - Data Format
    - Frequency 
    - Schema Evolution (API data specially)
    - PII data or normal data (Governance/Encryption)

- Business Use Case
    - Metrics to track - promotional activity - inventory ? shelf? 
    - How freq will you track ? Batch or real time? 
    - SLA (how much time will you take - 4 hours- rollback)

- Output
    - Target System 
    - Data Retention 
    - Refresh Frequency 
    - Historical Data  

**Summary**
1. Real Time Analytics 
2. Historical Analysis 
3. Scalability (Data + Processing)
4. Flexible data model

<h4> Building a solution </h4>

Incoming Data Sources 
1. API 
2. DB (SQL/ No SQL)
3. Files

- There can be a system that can pick from API and put into KAFKA or can also put directly from API to KAFKA in real time. 
- 6 hour sync in DB -> S3(Raw Layer) -> How will you pull data into S3? what will be Partitioning strategy? 
- Processing Layer -> (Medallion Architecture)
    - Metrics 1 - Near Real time (FLINK/ Spark Streaming) -> will go to Kafka -> UI (superset) or Dashboard
    - Metric 2 - incremental data (hourly or daily folder) after 6 hour sync -> Snowflake DWH (transient or permanent table / view materialised or normal) or go back to No SQL 
    - Business Usecase 
    - Data Modelling is done one time - Fact and dimension writing - distinct values in dimension - snapshot 
 
1. Why Kafka ?
2. Why SQL, No SQL?
3. Ingestion Layer - Pull Data 
4. Scheduling - Cron/ Airflow 
5. CI/CD (Gitlab/ Code coverage/ Test Cases/ Roll Back)
6. Exception Handling (on whole system, Trade off, Fault Tolerance, roll back )

### Drill Down 
1. API 
- Microservices 
- Event Driven Architecture 
- Pull/ Push Mechanisam 
- Authentication (JWT, SAML)
- Design Pattern -> LLD
- LLD (object , class interaction) how will you pull data from API or Files?
- Async Programming (very infreq, multi processing, multi threading)

2. Database (Most Focus)
- SQL, No SQL
- DB Internals 
- Volume Challenges (if volume is increasing day by day where to keep data DB or DWH)
- Optimization - Query and DB 
- Indexing/ Sharding/ Caching/ Materialized view - which column and what indexing
- ACID 
- CAP Theorem
- Constraints/ Normalization/ Denormalization
- Leader Follower Architecture 
- Connection Pull 

3. File Type 
- Parquet/ORC/CSV - which you have used why? what optimizations?
- structured vs unstructured( how will you process, schema evolution)
- Hudi, Iceberg, Delta Table 

4. S3 (80 TB per year - where should we keep? Business - how much do they need?)
- Cost Analysis 
    - Amazon - starting orders in cache - earlier year they keep in different layer 
    - Logs - Purge because volume is too much - 6 months - machine generated 
- DataLake vs Delta Lake 
- Data at Rest Encryption 
- Partitioning (File and DB)

5. Kafka 
- Backpressure 
- offset management ( where it is stored, how, how can we bring it back)
- Broker/ Producer/ Consumer 
- Kafka Connect 
- Topic and its management(why, when replication)
- Auto Commit and Linger Time 
- Exactly once record process( Failure overcome, so that it is dropped, how kafka manages this? there should be no duplication, how will you resolve this?)
- Failure Overcome 
- Replication 

6. Spark Streaming 
If you will you use Kafka , you need streaming, either you use Microservice Architecture or Flink 
- Flink vs Spark Streaming 
    - Late Data Arrival
    - Sliding window 
    - Checkpointing
    - commits
    - stateful vs stateless
    - event time - one semantics at the time of generation - when data arrived or producer system generated data and why, fault tolerance
    - Fault Tolerance  Question: You have Kafka with retention of 6 hour for logs - Failure of Spark Streaming, retention is over- now how will you ensure logs are there? will you increase retention? - linked in post suggested 3 days - but kafka cluster will be full
    - Performance optimization 
    
7. Processing layer (Not much focus here)
- Dimensional Modelling 
- Lakehouse Architecture 


8. Spark 
- on which platform will you run this ?
- data transformation
- all questions


9. Medallion Architecture
- GOLD Layer 
    - PII data (RBAC Permission) row level and column level 
    - How will you put encryption - UDF in spark - Encryption logic - how to decrypt 
    - Unity Catalog( Databricks)
    

Dimension Modelling 
- ER diagram Practice 
- Fact and Dimension Table 
- Ecommerce (user, sales, inventory)
- Ride sharing App 
- Finance Domain (credit card details) 
- Health Domain 

10. Scheduling/ Orchestration 
- Airflow 
- Internal Working 
- Type of executor 
- DAG/ TASK
- operators/ Sensor
- Custom operator 
- Xcom
- Backfill 
- Pools
- Automation/ Microservices

11. Docker & Kubernetes 


Not Required
- Login management 
- CDN 
- Tokenisation 
- Maps 
- Ride Sharing on Data Side not on Application Side 

</details>


#### Questions 

1. How would you make sure API calls with rate limits ?
2. Large volume of data in S3 which cannot be pulled directly into system - how will you proecess it ?