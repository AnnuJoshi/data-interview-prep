# System Design - 60 minutes

Overview: This interview covers services architecture
and systems design. You’ll be assessed on your ability to
design a high-level data E2E system while diving deep
into areas like data ingestion, data processing, scalability,
and the data persistence layer. You’ll be evaluated on
your ability to come up with efficient solutions to
open-ended problems by applying your knowledge of
systems theory and product development. This interview
will be conducted using an online whiteboard tool. Our
preferred platform for a virtual whiteboard is HackerRank,
however if there is a tool with similar functionality that you are more comfortable with then please
come prepared with it.

What they looking for:\
● Structure: Ensure you take a systematic approach to building your solution and that you
articulate what you are trying to do and why you are trying to do it.\
● Comprehensiveness: Ensure your approach covers all aspects of the requirements and
tackles any potential edge cases.\
● Feasibility: Ensure your solution is practical and could realistically be implemented.\
● Scalability: Ensure your solution has the capacity to scale as we increase users or
broaden the problem requirements.

Tips:\
● Clarifying Questions: Before jumping in to designing your solution, ensure that you
understand all aspects of the given requirements.\
● Trade Offs: Understand the various tradeoffs of the components in your system and
explain them to your interviewer.\
● Articulate Thought Process: If faced with difficulties, explain your solution approach and
reasoning. This transparency enables interviewers to provide valuable hints and insights.



## Experiences

<details>
<summary> Online </summary>

- You have to track user metrics - how will you develop solution for it - real time vs batch discussion 
Prev Project discussions - data modelling - data collection - transformation - where we will write them? what metrics on real time what batch ? what frequency? - dashboard refresh - frequency 
streaming focussed than batch(Manish Kumar Youtube)

(Blind)
- For system design interview. Question was something like hosting a service then backend db design for services and analytics design for data getting logged.
- Business round was more like business use case specific if there is drop in sales. How will you figure out? What data sources to look for ? Then what will be steps.
- Managerial round was on day to day activities. What’s the architecture? How do you handle conflicts? What were initiatives? What mistakes you made? What did you learn from those?

</details>


## Preparation 

<details>
    <summary>  Resources  </summary>

1. [System Design for Data]([https://www.youtube.com/watch?v=OWeQ_gCNe4k) 
2. System design Interview by Alex - Book
3. Design Data Intensive Application - Book
4. Youtube Videos - Arpit Bhayani, Gaurav Sen 
5. [Medium (Read) - Streamimg Data Pipeline](https://medium.com/@seancoyne/data-engineering-practice-system-design-question-and-solution-streaming-ad32562ba954)


### Videos 
1. [Building a data platform with open source: A $1M+ cost-saving journey](https://www.youtube.com/watch?v=WdQ1hlK57Ys)
</details>


<details>
    <summary>  High Level Componemts </summary>


Let's say you have to Design an e-commerce platfrom related to data side

- Clarify what you are trying to make 
    - What does e-commerce means ? Nykaa, Amazon
    - Are you asking to build transactional or analytical system?
    - Who will be the end consumer?
    - What is the scale of data, what is anticipated scale in upcoming 6 months or a year?
    - Existing systems or services which I should be aware of ?
    - What is the type of data? - Structured, unstructured, semi-structured?
    - Most important feature which I should start with ?

<h4> Back of envelope estimations </h4>

- Scale 
    - Power of 2 
        - 1 Kb 
        - 1 Mb 
        - 1 GB
        - 1 TB
        - 1 PB

- Latency - Microservices 
    - sending data over networks - compression(Json/Protobuff) - encoding (DDIA book)
    - Compression 
    - Multi region copy latency will matter

- Availability 
    - Pipeline criticality - no downtime - 99%
    - Pipeline Uptime
    - Tier 1, Tier 2, Tier 3 - Exception, ROllback implementation 

- Calculate storage requirements 
    - Total Data volume on a daily basis if you can tell
    - no of total users 
    - DAU or MAU 
    - Always round off the number to nearest 100
    - Daily 200 GB for 1 year = 200 * 400 = 8 * 10^4 = 80000 GB = 80 TB Data 
    - How much time you want to keep the data ? Purge? cold system? 
    - no. of seconds in a day ?

<h4> High Level Design - Draw.io </h4>

- Test Cases (CICD)
    
- Input 
    - Data Format
    - Frequency 
    - Schema Evolution (API data specially)
    - PII data or normal data (Governance/Encryption)

- Business Use Case
    - Metrics to track - promotional activity - inventory ? shelf? 
    - How freq will you track ? Batch or real time? 
    - SLA (how much time will you take - 4 hours- rollback)

- Output
    - Target System 
    - Data Retention 
    - Refresh Frequency 
    - Historical Data  

**Summary**
1. Real Time Analytics 
2. Historical Analysis 
3. Scalability (Data + Processing)
4. Flexible data model

<h4> Building a solution </h4>

Incoming Data Sources 
1. API 
2. DB (SQL/ No SQL)
3. Files

- There can be a system that can pick from API and put into KAFKA or can also put directly from API to KAFKA in real time. 
- 6 hour sync in DB -> S3(Raw Layer) -> How will you pull data into S3? what will be Partitioning strategy? 
- Processing Layer -> (Medallion Architecture)
    - Metrics 1 - Near Real time (FLINK/ Spark Streaming) -> will go to Kafka -> UI (superset) or Dashboard
    - Metric 2 - incremental data (hourly or daily folder) after 6 hour sync -> Snowflake DWH (transient or permanent table / view materialised or normal) or go back to No SQL 
    - Business Usecase 
    - Data Modelling is done one time - Fact and dimension writing - distinct values in dimension - snapshot 
 
1. Why Kafka ?
2. Why SQL, No SQL?
3. Ingestion Layer - Pull Data 
4. Scheduling - Cron/ Airflow 
5. CI/CD (Gitlab/ Code coverage/ Test Cases/ Roll Back)
6. Exception Handling (on whole system, Trade off, Fault Tolerance, roll back )


### Drill Down 
1. API 
- Microservices 
- Event Driven Architecture 
- Pull/ Push Mechanisam 
- Authentication (JWT, SAML)
- Design Pattern -> LLD
- LLD (object , class interaction) how will you pull data from API or Files?
- Async Programming (very infreq, multi processing, multi threading)

2. Database (Most Focus)
- SQL, No SQL
- DB Internals 
- Volume Challenges (if volume is increasing day by day where to keep data DB or DWH)
- Optimization - Query and DB 
- Indexing/ Sharding/ Caching/ Materialized view - which column and what indexing
- ACID 
- CAP Theorem
- Constraints/ Normalization/ Denormalization
- Leader Follower Architecture 
- Connection Pull 

3. File Type 
- Parquet/ORC/CSV - which you have used why? what optimizations?
- structured vs unstructured( how will you process, schema evolution)
- Hudi, Iceberg, Delta Table 

4. S3 (80 TB per year - where should we keep? Business - how much do they need?)
- Cost Analysis 
    - Amazon - starting orders in cache - earlier year they keep in different layer 
    - Logs - Purge because volume is too much - 6 months - machine generated 
- DataLake vs Delta Lake 
- Data at Rest Encryption 
- Partitioning (File and DB)

5. Kafka 
- Backpressure 
- offset management ( where it is stored, how, how can we bring it back)
- Broker/ Producer/ Consumer 
- Kafka Connect 
- Topic and its management(why, when replication)
- Auto Commit and Linger Time 
- Exactly once record process( Failure overcome, so that it is dropped, how kafka manages this? there should be no duplication, how will you resolve this?)
- Failure Overcome 
- Replication 

6. Spark Streaming 
If you will you use Kafka , you need streaming, either you use Microservice Architecture or Flink 
- Flink vs Spark Streaming 
    - Late Data Arrival
    - Sliding window 
    - Checkpointing
    - commits
    - stateful vs stateless
    - event time - one semantics at the time of generation - when data arrived or producer system generated data and why, fault tolerance
    - Fault Tolerance  Question: You have Kafka with retention of 6 hour for logs - Failure of Spark Streaming, retention is over- now how will you ensure logs are there? will you increase retention? - linked in post suggested 3 days - but kafka cluster will be full
    - Performance optimization 
    
7. Processing layer (Not much focus here)
- Dimensional Modelling 
- Lakehouse Architecture 


8. Spark 
- on which platform will you run this ?
- data transformation
- all questions


9. Medallion Architecture
- GOLD Layer 
    - PII data (RBAC Permission) row level and column level 
    - How will you put encryption - UDF in spark - Encryption logic - how to decrypt 
    - Unity Catalog( Databricks)
    

Dimension Modelling 
- ER diagram Practice 
- Fact and Dimension Table 
- Ecommerce (user, sales, inventory)
- Ride sharing App 
- Finance Domain (credit card details) 
- Health Domain 

10. Scheduling/ Orchestration 
- Airflow 
- Internal Working 
- Type of executor 
- DAG/ TASK
- operators/ Sensor
- Custom operator 
- Xcom
- Backfill 
- Pools
- Automation/ Microservices

11. Docker & Kubernetes 


Not Required
- Login management 
- CDN 
- Tokenisation 
- Maps 
- Ride Sharing on Data Side not on Application Side 

</details>


## Questions 

<details>
<summary> 1. How would you make sure API calls with rate limits ? </summary>

</details>

<details>
<summary>2. Large volume of data in S3 which cannot be pulled directly into system - how will you proecess it ? </summary>

</details>


<details>
<summary> 3. Imagine you have millions of IoT devices, emitting sensor data every second. Your goal is to collect this data and process it in real time to detect anomalies, and store both raw and processed data in a scalable data store. How would you design an end to end solution? </summary>

Questions that can be asked [5min - 10 min]

1. Main goal anomaly detection or other analytics needs (predictive maintenance, user behaviour analyis?)
2. Is data structured, semi-structured or unstructured?
3. Data comes in at constant rate or are there spikes in data ? 
4. Need to implement data encryption at rest or in transit?
5. Are there any specific compliance or regulatory requirements for storing sensor data, such as GDPR, HIPPA ?
6. How many devices do you expect initially, how quickly will that number grow? 

</details> <!-- Question 3. -->

<details>
<summary> 4. Let's say you encounter a Spark pipeline performing poorly, how do you go about identifying the issues and optimizing?</summary>

- Step 1: Understand the Symptoms
    Start by identifying the exact issue:
    1. Is the job slow overall or just certain stages?
    2. Is it failing or just taking too long?
    3. Is it CPU-bound, memory-bound, or I/O-bound?

    You can use:
    1. Spark UI (local or in your cluster manager like YARN/EMR/Databricks)
    2. Logs (driver and executor logs)
    3. Metrics dashboards (e.g., Ganglia, CloudWatch, Prometheus)

- Step 2: Profile the Spark Job
    Inspect the Spark UI:
    1. Stages & Tasks: Look for skewed stages, long-running tasks, or failed ones.
    2. Shuffle & Spill: Excessive shuffle and disk spill are red flags.
    3. Job DAG: Check if it's too complex or has unnecessary stages.
    4. Task Distribution: Are all cores utilized? Are some executors idle?

- Step 3: Review the Code and Transformations
    Look for:
    1. Wide transformations like groupBy, join, distinct, repartition — these trigger shuffles.
    2. UDFs — especially Python UDFs in PySpark — which are black-boxes to Spark.
    3. Collect or broadcast misuse.
    4. Unnecessary caching or missing persistence.

- Step 4: Check Data Issues
    1. Data Skew: One or a few keys have too much data (e.g., one country has 90% of rows). Fix via:
    - Salting the key
    - Aggregating before joining
    2. Small files: Can lead to too many tasks. Coalesce or compact beforehand.
    3. Data size: Check partition sizes — aim for 100–200MB per partition.

- Step 5: Tune Spark Configurations
    1. Memory Usage:
    - By default split, execution (for computation) and storage (for caching data)
    - Increase executor memory or storage fraction `(spark.executor.memory, spark.memory.storageFraction)`
    - Also, check spark.executor.memory to set the memory per executor—make sure it fits within your cluster’s node capacity.
    2. Parallelism:
        - Set `spark.sql.shuffle.partitions` appropriately (not default 200 for big data) based on your data size and cluster resources.
        - Tune `spark.default.parallelism`
        - 2-4 tasks per CPU core
        - How many cores are available in your cluster?
    3. Executor and Driver Settings: 
        - Set spark.executor.cores and spark.executor.instances to control how many executors run and how many cores each uses.
        For example, if a node has 16 cores, you might set 5 cores per executor to leave room for OS and other processes.
        - The driver (spark.driver.memory) also needs enough memory, especially for collecting results or running heavy operations.
    3. Garbage Collection: Long GC pauses can kill performance.
    4. Broadcast joins:
    If one side of a join is small (<10MB), enable broadcast joins
    Manually broadcast if Spark doesn’t auto-detect

Step 6: Infrastructure Considerations
Are your executors under-provisioned?
Is auto-scaling working effectively?
Are disk or network IO bottlenecks showing up in metrics?

- Step 7: Iterate & Test
    - After each change, re-run and compare:
        1. Job duration
        2. Shuffle size
        3. Stage/task run times
        4. Executor utilization

</details> <!-- Question 4. -->

<details>
<summary> 5. How do you manage the complexity of a pipeline with tons of transformations? </summary>
</details> <!-- Question 5. -->

<details>
<summary> 6. How do you go about testing this pipeline? </summary>
</details> <!-- Question 6. -->


<details>
<summary> 7. Design shopping cart for website </summary>

1. What is the user count ? 500 K 
2. Active user count ? 100K
    - system needs to be scalable 
3. Traffic spikes in the year or month? 
4. Stateless or stateful (state is maintained on refresh) - cart should be stateful 
5. Region (US, Europe, APAC) for data governance 
6. On prem, on cloud 

### Design 
 
- Load balancer for spikes 
- AutoScaling for compute 

- caching mechanism 
    1. Redis caching, a database with large cache, super performant 
        - Cons manage an extra component and cost
- VPC router - will route to the right region 

Data Pipeline for 
1. Data Source 
2. Batch or Live 
3. Frequency ? 
4. Data Size 
   - 450 GB /month 
5. Already have an system in place? you need to move to cloud from spark
6. Data customers? 
    - BI users
    - DS (ML model)
    - Operational User 

Design 
1. Landing Area - S3 - all data comes here
2. Spark based processing system in AWS? 
3. Store in S3 again 
4. Snowflake - serverless for SQL part 
5. Notebooks for ML people 
6. Orchestration - Airflow 

</details>  <!-- Question 7. -->

<details>
<summary> 8. Design ETL pipeline to ingest data from multiple external APIs 
Handle Schema Evolution </summary>

Airflow @daily vs @once triggered 
Slow sql to optimize
failure handling, retry logic , partitioning 
how will you handle large file in GBs
asked to find top 10 user by event frequency - constraint was optimized for memory
what if streaming data type change mid way
generator functions 

System Design 
Real time Data Pipeline for click stream events 
Ensure fault toleranc 
where deduplication logic 
store 1 billion records 
z ordering 

Behaviour 
- You take full ownership of failing project 
- What do you if deadline is missed because of your code 

</details>

<details>
<summary> 9. Netflix Clickstream Data Pipeline</summary>

[Design Data pipeline for Netflix metrics monitoring for click stream or playback data](https://www.youtube.com/watch?v=53tcAZ6Qda8)

### Product Metrics

#### Questions 
1. What metrics are you looking for? You have to define
    - User engagement from click stream 
        - User churn - if they are not visiting, what is their interest
        - Path Analysis - navigation path customer is taking 
          - set a target that they will click on the promotion 
          - what is blocking them? is it too long mey be 7-8 clicks?
        - Behavior Profiling - 
    - Playback
        - trending series
        - when they click on pause? because most people binge watch if it is interesting 
        - time it take for a user to watch 1 hour show may be they are taking 3 hours, can use this for recommendation
2. Keep the solution Generic - do you want to use some specific technologies?

### Pipeline Design 

1. Data Capture 
2. Streaming/Batch 
3. Processing 
4. Storage 
5. Analytics

6. Metrics 
   - Number of subscribers = 200M 
   - 50% Active Daily = 100 M
   - 1 day = 100k seconds
   - 1 day = 100M/100k = 1000 users/sec , cannot assume for every sec 
   - 80% traffic-20 of time rule = 100M users/20K sec = 5000 users/sec
   - user generate 10 events per sec = 50K events per sec - this can be more than this when a popular series come in - traffic spikes 

- Geographical distribution is something we should take into account for Netflix's scale -> Cloud front distribution

- Push and pull models 
    - Push - agents running on server - quickly overwelhm your infra - you don't have to pull
    - Pull - You decide when you want to poll for more data, your insights might be lost sometimes if you are polling late - timing is critical for some promotions

- API Gateway - where servers can push data to 
- Lambda -> you can collect and preprocess data 
- Kakfa -> has buffer, if consumer is not online it can pick up - can store data for max 7 days
- Spark -> distributed computing platform - spark streaming module which can plug into Kafka to extract data - works in microbatches not sub sec latency 
- Flink -> real time distributed computing platform - milli second latency
- DataLake Why? 
    - Store in ObjectStore 
    - Raw Layer 
    - Processed 
    - Need data stored over time - for analytics 
    - in S3 you can run queries directly using Athena, straightway 
    - After Flink you can put in NO SQL DB like Dyanamo DB - no schema - fast read/writes 
    - If you use Relational DB for storing it will be the bottleneck - pipeline will be as fast as DB 

1. What is the difference between row based file format & columnar file format? And why columnar-based file formats such as parquet or orc are favoured for analytics?
    - Row-Based File Formats store data in a sequential, row-by-row manner. Think of it like a spreadsheet where each row represents a complete record, and all the values for that record (across all columns) are stored together. Examples include CSV, JSON, and traditional relational database storage formats. When you read data from a row-based format, you typically access all the columns for a given row at once, even if you only need a subset of the data.

    - Columnar File Formats, on the other hand, store data by column rather than by row. Each column's data is stored together, separate from the other columns. Formats like Apache Parquet and ORC (Optimized Row Columnar). When you read from a columnar format, you can efficiently access just the specific columns you need without loading the entire dataset into memory.

    - Key Differences
        1. `Storage Organization`: Row-based formats keep all data for a single record together, while columnar formats group data by column, allowing for more efficient access to specific fields across many records.
        2. `Read Efficiency`: In row-based formats, querying a single column often means scanning through entire rows, loading unnecessary data. Columnar formats let you read only the columns you're interested in, which is much faster for analytical queries.
        3. `Compression`: Columnar formats often achieve better compression because data within a `single column tends to be more similar (e.g., a column of dates or numbers)` compared to `mixed data types in a row`. This reduces storage size and speeds up data transfer.
        4. `Write Performance`: `Row-based formats are generally better for write-heavy workloads` (like transactional systems) because `appending a new record is straightforward`. Columnar formats can be `slower for writes since data needs to be organized by column`.
    
    - Why Columnar Formats Like Parquet and ORC Are Favored for Analytics
        which often involve aggregating or analyzing specific columns across large datasets (think data warehouses or big data processing). Here's why they shine in this context:

        1. Selective Column Access: target only a subset of columns (e.g., summing sales figures or filtering by date). Columnar formats allow systems to read just the relevant columns, skipping irrelevant data, which drastically reduces I/O and speeds up query execution.
        2. Better Compression and Encoding: Since columnar data is more homogeneous, it compresses better using techniques like run-length encoding or dictionary encoding. For example, Parquet and ORC can store repeated values efficiently, saving space and improving read performance.
        3. Partitioning and Predicate Pushdown: Formats like Parquet support metadata and partitioning, enabling query engines (like Apache Spark or Hive) to skip entire chunks of data that don't match query conditions. This means less data is scanned, further boosting performance.
        4. Scalability for Big Data: In distributed systems, columnar formats work well with parallel processing frameworks. They allow tasks to be split by column or data block, making it easier to handle massive datasets efficiently.
       
        In contrast, `row-based formats are more suited for transactional systems (OLTP) where entire records are frequently inserted, updated, or retrieved as a whole`.

2. Different compression techniques such as snappy, biz2 and LZO. And which one to choose?
    Compression techniques are used to reduce the size of data, which helps save storage space and can speed up data transfer or processing, especially in big data environments. Each method has trade-offs in terms of `compression ratio (how much it reduces size)`, `speed of compression/decompression`, and `computational overhead`.
    1. Snappy:
        - Snappy is a compression library developed by Google, designed for high-speed compression and decompression with a `focus on low latency`. It `prioritizes speed `over achieving the smallest possible file size.
        - Compression Ratio: Moderate. It doesn't compress data as tightly as some other algorithms, but it still reduces size significantly for many data types.
        - Speed: Very fast for both compression and decompression, making it ideal for real-time or near-real-time applications.
        - Use Case: Often used in big data frameworks like Apache Hadoop, Spark, and Kafka, where quick compression/decompression is critical during data processing or streaming.
        - CPU Usage: Relatively low, as it avoids complex algorithms to maintain speed.`
    2. Bzip2:
        Bzip2 is a compression algorithm that uses the Burrows-Wheeler transform and Huffman coding to achieve high compression ratios. It’s `more focused on minimizing file size` than on speed.
        - Compression Ratio: High. It typically produces smaller files compared to Snappy or LZO, especially for text-heavy data.
        - Speed: Slower for both compression and decompression compared to Snappy or LZO. 
        - Use Case: Suitable for scenarios where storage space is a primary concern and speed is less critical, such as archiving data or compressing files for long-term storage.
        - CPU Usage: Higher than Snappy or LZO due to its complex algorithm, which can be a bottleneck in resource-constrained environments.
    3. LZO (Lempel-Ziv-Oberhumer):
        - LZO is a lightweight compression algorithm focused on speed, similar to Snappy, but with a slightly different balance between compression ratio and performance.
        - Compression Ratio: Moderate, often comparable to Snappy, though it can vary depending on the data type. It’s generally not as tight as Bzip2.
        - Speed: Very fast decompression, often faster than Snappy, but compression speed can be slightly slower than Snappy in some cases.
        - Use Case: Commonly used in real-time systems or embedded environments `where decompression speed is critical`, such as in file systems (e.g., SquashFS) or data transfer protocols.
        - CPU Usage: Low to moderate, designed to be lightweight and efficient even on less powerful hardware.
        
        Which One to Choose?
        - Choose Snappy if you’re working in a big data or analytics environment (e.g., with Hadoop, Spark, or Kafka) where speed is critical, and you’re dealing with frequent read/write operations. Snappy is often the default choice for columnar file formats like Parquet or ORC in these systems because it balances speed and compression well for intermediate data processing. It’s ideal when you need low latency and can afford slightly larger file sizes.
        - Choose Bzip2 if your `primary goal is to minimize storage space` and you’re not constrained by time or CPU resources. 
        - Choose LZO if you need extremely fast decompression and are working in a system where read performance is more important than write performance.
    
    - Additional Considerations
        1. Data Type: The effectiveness of each algorithm can depend on the nature of your data. For example, Bzip2 excels with text data due to its ability to exploit patterns, while Snappy and LZO are more general-purpose.
        2. Framework Compatibility: If you’re using tools like Apache Spark or Hadoop, check their default or recommended compression codecs. Snappy is widely supported and often the default in these ecosystems for a reason.
        3. Splittability: In distributed systems, ensure the compression method supports splitting compressed files for parallel processing. Snappy and LZO are typically splittable when used with formats like Parquet, while Bzip2 is splittable by design but slower, which might negate the benefit.

</details>