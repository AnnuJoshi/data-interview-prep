### Data Modeling Case Study Prep 

Overview: This scenario-based interview will focus on your `product sense and data modeling skills`. You’ll be assessed on
data literacy,\
data warehousing,\
ETL implementation,\
collaboration skills, 
and your product sense. 

You will be asked to **design some metrics** and
create an **entity-relationship diagram**, and will create an
ETL flow using the data model you designed. You will be
evaluated not only on the strength of your solution, but
also your approach and level of autonomy in working
through the problem. You will be collaborating with the
interviewer through a Google Doc. You will be asked to
write SQL, but the code does not need to have perfect syntax.

What we’re looking for:\
● Problem Solving: We want to get a sense of how you think through ambiguous problems!
We’re not necessarily looking for the perfect, most efficient solution immediately, but want
to see how you iterate to get there.\
● Communication: Can you explain your thought process and how you arrived at a
solution? Can you receive feedback throughout the interview and course correct? We are
looking for effective communication throughout the duration of the interview, especially if
your solution isn’t working the way you intended it to.

Tips:\
● Interactive Interview: Be prepared for a highly interactive session. Asking clarifying
questions and seeking a comprehensive understanding of the problem is crucial.\
● Articulate Thought Process: If faced with difficulties, explain your solution approach and
reasoning. This transparency enables interviewers to provide valuable hints and insights.\
● Focus on the Question: The interview is designed to guide you step by step towards the
final solution. Please do not jump ahead to solutioning in the earlier sections of the
interview, where we would still be discussing the scenario.\
● Scalability: We encourage you to think about how your solution would scale with
increasing data volume and complexity.

Tips 
1. Partitioning and indexing strategies
2. cardinality explanation 
3. How queries infuence scham design - design for read efficiency 


### Resources
<details>
<summary> <a href="https://www.tryexponent.com/courses/data-engineering/data-modeling-interviews/data-modeling-intro">1. Exponent</a>
</summary>

- Identifying business requirements and technical constraints
- Designing a dimensional model for a data warehouse with appropriate fact and dimension tables.
- Design trade-offs: How you weigh different design options and articulate their pros and cons.
- Performance considerations: Your understanding of how to optimize the model for query performance and scalability.
- Creating an ER diagram of your solution, typically including 3-5 tables
- Adapting your data model to new requirements or feedback.


Key areas of assessment include:

1. Defining appropriate grain for fact tables
2. Identifying relevant dimensions and facts
3. Handling slowly changing dimensions
4. Considering query patterns and performance implications
5. Addressing data volume and scalability concerns
6. Articulating design choices and trade-offs

</details>

### Preparation 
<details>
<summary> Different modeling approaches</summary>
Understand trade-offs: Study the pros and cons of different modeling approaches, such as star schemas vs. snowflake schemas, or normalized vs. denormalized designs.

It’s about understanding the bigger picture:
1. Diagnosis: If DAU or MAU drops, how would you diagnose it? Which metrics would you check? What questions would you ask?
2. Product Strategy: How would you improve user retention? What data would help drive that decision?

</details>


<details>
<summary> Dimensional Modeling</summary>
Ensure a thorough understanding of dimensional modeling, including fact tables, dimension tables, and slowly changing dimensions


### Fact data Model 
- Transaction
- Periodic Snapshot Fact Table 
- Accumulating Snapshot
- Bridge Tables (Factless Fact Tables)

</details>


### Experience
<details>
<summary> Online  </summary>

1. Fixed question - wearable device track, query writing, user avg, how many days login, logout date, BI idea, build data modal - how will keys interact? (Manish Kumar Youtube)
2. You're a PM at a food delivery app where conversion rates have declined over the past week. How would you investigate the causes? (Conversion: From users browsing to placing orders.)
3. [On DoorDash, there are missing item and wrong item issues for deliveries. How would you analyze each of them?](https://www.tryexponent.com/questions/4862/doordash-order-issue-analysis)
4. Growth, scaling challenges for setting up a new DoorDash campaign (Glassdoor)

</details>

## Questions 
<details>
<summary> 1. How to model CDC ?</summary>
</details>

<details>
<summary> 1. How to model SDC ?</summary>
</details>

## Examples 
<details>
<summary> 1. Real Time Walmart System Design </summary>

Initially business explained by interviewer - (They said transactional but expectation was analytical)

1. Asked for DAU = 20 million 
2. 86400 sec in a day - rounded to 10^5  (we know this)
3. 20* 10^6 / 10^5 = 200 users per sec 
4. they are writing 5 things so 200*5 = 1000 writes/sec in DB
5. Request will be 2.5 times of this = 2500 

Transactional System 
- Latency
- Consistency

Q- He asked, Why are you doing all this?
A- To asses how large the data will be, which DB should be used, latency, schema evolve, sql or no sql should be used? 
He asked for tables - wanted to go for analytical system - dimensional modelling 

Q- I asked, Which portion you want to make ? Order 
A- I said, okay Order and user 

Order Table 
- user_id (FK)
- order_id (PK)
- order_date
- order_amount
- quantity
- discount

User Table 
- user_id (PK)
- login_date 
- logout_date

Q: He asked Where are Shipment details, cart, whishlist, inventory? 
Q: Asked One to many or many to many mapping

Shipping Table (Acummulating Snapshot) - to analyse where clogging is happening in supply chain 
                                       - All dates as we know multiple steps in a shipping process
- order_id
- order_date 
- dispatch_date
- shipment_date 
- out_for_delivery_date 
- delivered_date

Cart Table 
- user_id
- product_id
- cart_added_date

Product Table 
- product_id (PK)
- seller_id

Seller Profile Table 
- seller_id
- seller


Q: If we have to search in NoSQL, user will search a query in amazon search bar, how will you do? 
A: Elastic Search - text search easier in it 
Product name - if user make a typo, we have to show all relevant products 
Let's say user type datawarehouse then we have to show him all books containing that keyword 

Q: How will you implement this? Write query for this
A: 

Q: How will user journey happen for this search and how will you return to user? He wanted to test microservices 
A: User_login -> authentication -> success -> search button /api/v1/search/ -> payload(keyword) -> will keep pools and choose one to make elastic connection -> pass payload to query -> response of the page url -> parse on the user page (encoding json, protobuff, Atlassian recently changed)

LLD 
Q: Write code to implement this 
app.post("/api/v1/search")
 Search.search()

Tried to use Design Pattern in this 

ABC is abstract base class in python 
whosoever inherit this class will have to implement these functions 

class abs_methos(ABC):
    def __init__(self):
        self.abc = None

    def search():
        pass

    def response():
        pass

class search(abs_method):
    def __init__(self):
        self.config = config 
    
    def search("text_serach"):
        # make connection
        
        return result 

</details>

<details>
<summary> 2. Wearable devices for delivery partners </summary>
 
 - To track activity metrics like steps taken, distance traveled, and active time during deliveries. 
 - The goal is to improve `Dasher efficiency`, `optimize delivery routes`, and enhance overall health and safety.
 - Design a data model to store and analyze this wearable device data, define key metrics for success, and outline how the data will be processed. 

### Answer
1. Entities
    - Dasher
        - dasher_id: VARCHAR(50), PRIMARY KEY, Unique identifier for each Dasher
        - name: VARCHAR(100), NOT NULL, Dasher’s full name
        - start_date: DATE, NOT NULL, Date joined DoorDash
    - Wearable
        - wearable_id
        - dasher_id as FK 
    - Activity_Log
        - log_id
        - wearable_id as FK
        - timestamp
        - lat
        - long
        - steps
        - distance_travelled
        - heart_rate
    - Order
        - order_id
        - dasher_id as FK
        - customer_id as FK
        - restaurant_id as FK
        - timestamps
        - rating
    - Customer
    - Restaurant
    - External_Factors
    - I used a separate Activity_Log table for scalability since wearable data is high-frequency
2. Relationships:
    - Dasher   1:N Wearable (one Dasher can have multiple devices over time).
    - Wearable 1:N Activity_Log (one device logs many entries).
    - Dasher   1:N Order (one Dasher handles many orders).
    - Order    N:1 Customer 
    - Order    N:1 Restaurant (many orders per customer/restaurant).
3. primary keys (PK) and foreign keys (FK) on connectors

Invite Questions: 
1. Does this align with DoorDash’s needs? 
2. Are there other entities or relationships you’d suggest adding?

Mention
1. Scalability: Given DoorDash’s volume, I’d partition Activity_Log by date and index timestamp and wearable_id for fast joins. This handles millions of daily logs without performance hits.
2. Data Quality: I’d build ETL checks to flag invalid lat/long or missing timestamps before loading into these tables, ensuring reliable analysis.
3. Extensibility: This model can expand to include new wearable metrics (e.g., calories burned) by adding columns to Activity_Log, or new external data (e.g., real-time traffic APIs) via External_Factors.
4. Collaboration: I’d validate this with product teams to confirm fields like rating meet reporting needs, and with infra teams to align on warehouse constraints.

Adapt to Feedback: 
1. If the interviewer challenges a design (e.g., “Why not denormalize Activity_Log into Order?”), explain your reasoning (e.g., `“Normalization helps with wearable data volume and updates”`) and offer alternatives (`“We could denormalize for specific dashboards if query speed is critical”`).

### Generate Metrics and SQL 
Step 1: Define Objectives and Assumptions
- My assumptions here are:
    1. Timestamps are in a consistent format (e.g., UTC) and mostly non-null.
    2. Rating is on a 1-5 scale.
    3. Wearable data in Activity_Log is high-frequency but can be aggregated


1. Dasher Efficiency - Average Delivery Time per Dasher, and correlate with wearable activity data like distance_travelled.
    ```sql
    SELECT dasher_id, 
        AVG(EXTRACT(EPOCH FROM (delivery_time - pickup_time))/60) AS avg_del_time_minutes  -- convert to minute 
    FROM orders 
    WHERE order_time >= CURRENT_DATE - INTERVAL '30 days' --current date 
    AND pickup_time IS NOT NULL -- remember this
    AND delivery_time IS NOT NULL -- edge case 
    GROUP BY dasher_id
    ORDER BY avg_del_time_minutes
    ```
2. Customer Satisfaction - Delivery Time and Rating Correlation
   
   - Thresholds: I’ll bucket delivery times (<20 mins, 20-30, >30) to see rating trends rather than a binary ‘related’ label, addressing aggregation needs.

   - Regional Trends: JOIN with Customer to group by approximate region using lat/long rounded to a grid (e.g., 0.1 degree), addressing the cross-question on geography.
   
   - Scalability: Indexing on order_time and customer_id for joins; consider materializing regional grids if frequently queried.

   - Buckets delivery times and groups by approximate region (lat/long rounded for simplicity), showing average ratings per bucket and area. It reveals if faster deliveries consistently yield higher ratings and if geography plays a role (e.g., urban vs. rural delays)

        ```sql 
        SELECT 
        CASE 
            WHEN EXTRACT(EPOCH FROM (o.delivery_time - o.pickup_time))/60 < 20 THEN '<20 mins'
            WHEN EXTRACT(EPOCH FROM (o.delivery_time - o.pickup_time))/60 < 30 THEN '20-30 mins'
            ELSE '>30 mins'
        END AS delivery_time_bucket,
        ROUND(c.lat, 1) || ',' || ROUND(c.long, 1) AS approx_region,
        COUNT(*) AS total_orders,
        ROUND(AVG(o.rating), 2) AS avg_rating
        FROM orders o
        JOIN customer c ON o.customer_id = c.customer_id
        WHERE o.order_time >= CURRENT_DATE - INTERVAL '30 days'
            AND o.pickup_time IS NOT NULL
            AND o.delivery_time IS NOT NULL
            AND o.rating IS NOT NULL
        GROUP BY 
            CASE 
                WHEN EXTRACT(EPOCH FROM (o.delivery_time - o.pickup_time))/60 < 20 THEN '<20 mins'
                WHEN EXTRACT(EPOCH FROM (o.delivery_time - o.pickup_time))/60 < 30 THEN '20-30 mins'
                ELSE '>30 mins'
            END,
            ROUND(c.lat, 1) || ',' || ROUND(c.long, 1)
        ORDER BY delivery_time_bucket, avg_rating DESC;
        ```
4. Health and Safety: Identifying Overexertion via Heart Rate

    - Objective: Flag Dashers with high average heart_rate (>120 bpm) during shifts, check delivery count correlation.

        ```sql
        SELECT 
        d.dasher_id,
        DATE(al.timestamp) AS activity_date,
        ROUND(AVG(al.heart_rate), 2) AS avg_heart_rate_bpm,
        COUNT(DISTINCT o.order_id) AS deliveries_completed
        FROM dasher d
        JOIN wearable w ON d.dasher_id = w.dasher_id
        JOIN activity_log al ON w.wearable_id = al.wearable_id
        LEFT JOIN orders o ON d.dasher_id = o.dasher_id
            AND DATE(o.order_time) = DATE(al.timestamp) -- is okay to join with timestamp?
        WHERE al.timestamp >= CURRENT_DATE - INTERVAL '7 days' -- is this needed - should be order date
            AND al.heart_rate IS NOT NULL
        GROUP BY d.dasher_id, DATE(al.timestamp)
        HAVING AVG(al.heart_rate) > 120
        ORDER BY avg_heart_rate_bpm DESC;
        ```
5. Route Optimization Check: Comparing Actual vs. Straight-Line Distance
    
    Objective: Check if Dashers take efficient routes by comparing distance_travelled to straight-line distance
     - Sum distance_travelled from Activity_Log between pickup_time and delivery_time
     - compute straight-line distance using Haversine formula approximation with lat/long
     - approximate straight-line as a baseline (real routes aren’t straight due to roads)
     - Scalability: Location data is voluminous; limit to recent orders and optimize joins
        ```sql
        SELECT 
            o.order_id,
            o.dasher_id,
            ROUND(SUM(al.distance_travelled), 2) AS actual_distance_km,
            ROUND(
                6371 * ACOS(
                    COS(RADIANS(r.lat)) * COS(RADIANS(c.lat)) * 
                    COS(RADIANS(c.long) - RADIANS(r.long)) + 
                    SIN(RADIANS(r.lat)) * SIN(RADIANS(c.lat))
                ), 2
            ) AS straight_line_distance_km,
            CASE 
                WHEN SUM(al.distance_travelled) > 1.5 * (6371 * ACOS(
                    COS(RADIANS(r.lat)) * COS(RADIANS(c.lat)) * 
                    COS(RADIANS(c.long) - RADIANS(r.long)) + 
                    SIN(RADIANS(r.lat)) * SIN(RADIANS(c.lat))
                )) THEN 'Inefficient'
                ELSE 'Efficient'
            END AS route_efficiency
            FROM orders o
            JOIN restaurant r ON o.restaurant_id = r.restaurant_id
            JOIN customer c ON o.customer_id = c.customer_id
            JOIN activity_log al ON o.dasher_id = al.wearable_id::text -- Adjust linkage as needed
                AND al.timestamp BETWEEN o.pickup_time AND o.delivery_time
            WHERE o.order_time >= CURRENT_DATE - INTERVAL '7 days'
                AND o.pickup_time IS NOT NULL
                AND o.delivery_time IS NOT NULL
            GROUP BY o.order_id, o.dasher_id, r.lat, r.long, c.lat, c.long
            ORDER BY actual_distance_km DESC
            LIMIT 100;
        ```

</details>
<details>
<summary> 3. Data Modelling for Airbnb </summary>

[Video](https://www.youtube.com/watch?v=vsBo2CzJHeY)

1. Analytical or Transactional ? 

2. Choosing for Star Schema - denormalised data - efficient for reads
                            - not optimized storage
   [Star and Snowflake Schema](https://medium.com/@seancoyne/cracking-the-data-modeling-interview-part-4-designing-efficient-data-structures-625a78d5fe6c)

3. What Metrics are we trying to solve for? 
    - Customer Satisfaction or Enagement 
    - Pricing - Revenue - Trend in terms of reservation 

4. DataSet Identification 
    - review_fact 
    - bookings_fact 
    - revenue_fact -> more business from certain areas
    - customer_dim 
    - listing_dim

5. Dive deeper into fields to calculate metrics 
    - review_fact
        - review_id (PK)
        - booking_id (FK to booking_fact) -- is imp so that only users who have stayed at airbnb are able to leave a review
        - listing_id (FK to listing dim)
        - user_id (FK to customer dim)
        - review_date (FK to date_dim) -- date or timestamp
        - review_text
        - rating (scale 1-5)
        - response_time 
    - listing_dim
        - listing_id (PK)
        - location 
        - listed_date
        - num_of_rooms 
        - host_id (FK to host dim)
        - property_type (house, apartment)
    - host
        - host_id (PK)
        - name 
        - email 
        - superhost 
        - joindate 
        - country
        - phonenumber 
        - response_rate 
    - customer_dim 
        - user_dim (PK)
        - name 
        - contact 
        - email
        - country 
        - joining_date 
    - date_dim 
        - date (PK)
        - month
        - year
        - quarter 
        - weekofyear 
        - isholiday
    - booking_fact 
        - booking_id (PK)
        - listing_id (FK to listing_fact)
        - user_id (FK to customer_dim) 
        - host_id (FK to host)
        - booking_datetime
        - checkin 
        - checkout 
        - number_of_nights
        - cost 
        - booking_source -- invaluable to analyse which
        - number_of_guests
        - discount 
        - status (cancelled/confirmed/pending)
        - cancellation_reason 

- All date columns linked to date_dim 
- Two Facts in center and all dims around 
- Cost associated with a booking is - how many guests - analyse revenue - booking trends

- for pricing we will have revenue_fact 
    - revenue_id (PK)
    - listing_id (FK)
    - booking_id (FK to booking_fact)
    - revenue_generated 
    - payment_method
    - currency 
    - payment_date 

Question - How will calculate revenue by city?
- city_dim 
    - location_id 
    - city
    - state 
    - country 
    - pincode 

```sql
    select city, sum(total_revenue) as total_revenue 
    booking_fact
    join by 
    listing_dim on booking_fact.location_id = listing_dim.location_id
    on 
    group by city
```
```sql
-- host by cancellation


```

#### Tradeoffs/Challenges - for scale of Airbnb 
- Booking fact and revenue fact enormously - so DB will slow down how we are partitioning it - so that we have guardrails around reading data 
- Partitioning ? date dim if we care about recent data 
- if we can care about Geography then partition on location 
- PK are also good candidates for partition - not making specific partition a hot partition 
- Quicker for reading, not skeweing our data and not prone to failure while writing 

#### Anything else you want to touch upon 
- review fact - we will have to deal with unstructured data, will have to apply some text processing techniques, can probably store separately - sentiment analysis 
- competitor pricing - pricing performance with other companies - dynamic pricing

</details>

<details>
<summary> 4. Interview exp </summary>

[Medium Blog](https://blog.dataengineerthings.org/what-its-like-to-interview-at-doordash-for-a-data-engineering-role-a53e8da82485)

[Medium Blog](https://medium.com/@sandeep_nutakki/doordash-senior-data-engineering-prescreening-and-onsite-interview-prep-guide-553d578d3fb1)

- What happened: This round revolved around designing data models based on real-world analytical queries.

- Scenario: Given a set of business questions (e.g., “Find top customers by repeat orders,” “Track conversion by funnel stage”), I had to write SQL queries and design a schema to support them efficiently.
- Focus Areas:
    Choosing appropriate primary/foreign keys
    Handling one-to-many and many-to-many relationships
    Partitioning and indexing strategies
    SQL Portion: A few medium-complexity queries involving joins, aggregations, and GROUP BY logic were asked to validate the proposed schema.

- Understand how queries influence schema design — design for read-efficiency.

- Be ready to explain key relationships and cardinality.

    ```sql 
        -- Top customers by repeat orders (customers with 2+ orders)
    SELECT 
        customer_id,
        customer_name,
        COUNT(*) as total_orders,
        COUNT(*) - 1 as repeat_orders,
        MIN(order_date) as first_order_date,
        MAX(order_date) as last_order_date,
        SUM(order_amount) as total_spent,
        AVG(order_amount) as avg_order_value
    FROM orders o
    LEFT JOIN customers c ON o.customer_id = c.customer_id
    WHERE customer_id IS NOT NULL
    GROUP BY customer_id, customer_name
    HAVING COUNT(*) >= 2  -- Only customers with repeat orders
    ORDER BY total_orders DESC, total_spent DESC
    LIMIT 20;
    ```
    
    ```sql
    -- Customers ranked by repeat order frequency
    WITH customer_metrics AS (
        SELECT 
            customer_id,
            customer_name,
            COUNT(*) as order_count,
            COUNT(DISTINCT DATE_TRUNC('month', order_date)) as active_months,
            DATEDIFF('day', MIN(order_date), MAX(order_date)) as customer_lifespan_days,
            SUM(order_amount) as lifetime_value
        FROM orders o
        LEFT JOIN customers c USING (customer_id)
        GROUP BY customer_id, customer_name
        HAVING order_count >= 2
    )
    SELECT 
        *,
        ROUND(order_count::FLOAT / NULLIF(active_months, 0), 2) as orders_per_month,
        ROUND(lifetime_value / order_count, 2) as avg_order_value
    FROM customer_metrics
    ORDER BY order_count DESC, lifetime_value DESC;
    ```

</details>


<details>
<summary> Revision </summary>

1. Can you write the order conversion rate query? (App opens → Completed orders)
    ```sql
    WITH user_sessions AS (
    SELECT 
        user_id,
        session_id,
        MAX(CASE WHEN action = 'app_open' THEN 1 ELSE 0 END) AS had_app_open,
        MAX(CASE WHEN action = 'order_completed' THEN 1 ELSE 0 END) AS had_order_completed
    FROM user_events
    WHERE event_date >= CURRENT_DATE - INTERVAL '30 days' -- imp time interval
    GROUP BY user_id, session_id -- need to track session value as well 
    )
    SELECT 
    SUM(had_order_completed)::DECIMAL / SUM(had_app_open) AS session_conversion_rate
    FROM user_sessions
    WHERE had_app_open = 1;
    ```
2. Multi funnel analysis 
    ```sql 
    WITH session_funnel AS (
    SELECT 
        user_id,
        session_id,
        MAX(CASE WHEN action = 'app_open' THEN 1 ELSE 0 END) AS step_1_app_open,
        MAX(CASE WHEN action = 'restaurant_selected' THEN 1 ELSE 0 END) AS step_2_restaurant_selected,
        MAX(CASE WHEN action = 'item_added_to_cart' THEN 1 ELSE 0 END) AS step_3_cart_add,
        MAX(CASE WHEN action = 'order_completed' THEN 1 ELSE 0 END) AS step_4_order_completed
    FROM user_events
    WHERE event_date >= CURRENT_DATE - INTERVAL '30 days'
    GROUP BY user_id, session_id
    ),
    funnel_metrics AS (
    SELECT 
        -- Step counts
        SUM(step_1_app_open) AS total_app_opens,
        SUM(CASE WHEN step_1_app_open = 1 AND step_2_restaurant_selected = 1 THEN 1 ELSE 0 END) AS restaurant_selections,
        SUM(CASE WHEN step_2_restaurant_selected = 1 AND step_3_cart_add = 1 THEN 1 ELSE 0 END) AS cart_adds,
        SUM(CASE WHEN step_3_cart_add = 1 AND step_4_order_completed = 1 THEN 1 ELSE 0 END) AS order_completions,
        
        -- Overall end-to-end
        SUM(CASE WHEN step_1_app_open = 1 AND step_4_order_completed = 1 THEN 1 ELSE 0 END) AS end_to_end_conversions
    FROM session_funnel
    )
    SELECT 
    -- Step-by-step conversion rates
    restaurant_selections::DECIMAL / total_app_opens AS conversion_open_to_restaurant, -- REM THIS the rate num and deno
    cart_adds::DECIMAL / restaurant_selections AS conversion_restaurant_to_cart,
    order_completions::DECIMAL / cart_adds AS conversion_cart_to_order,
    
    -- Overall conversion rate
    end_to_end_conversions::DECIMAL / total_app_opens AS conversion_open_to_completed,
    
    -- Raw numbers for context
    total_app_opens,
    restaurant_selections,
    cart_adds,
    order_completions,
    end_to_end_conversions
    FROM funnel_metrics;
    ```

    - Drop-off points: Which step loses the most users?
    - Step efficiency: Is the problem getting people to select restaurants or complete checkout?
    - Overall health: End-to-end conversion rate trend

    Real interview follow-up: "If you see conversion_restaurant_to_cart dropped 15% last week, what would you investigate first?"
    ✅ Item Availability - Are popular items out of stock?
    ✅ Restaurant Pricing - Did restaurants raise prices?
    ✅ Menu Availability - Are restaurants going offline more often?
    ✅ Search Relevance - Are we showing restaurants users don't want?

    ```sql
     -- Check if out-of-stock rate increased
    SELECT 
    DATE_TRUNC('week', event_date) as week,
    AVG(CASE WHEN action = 'item_unavailable_click' THEN 1.0 ELSE 0 END) as unavailable_rate
    FROM user_events 
    WHERE action IN ('item_click', 'item_unavailable_click')
    and updated_at >= CURRENT_DATE - INTERVAL '4 weeks'
    GROUP BY 1 ORDER BY 1;
    ```
    ```sql 
    -- Average item price trend by week
    SELECT 
    DATE_TRUNC('week', updated_at) as week,
    AVG(price) as avg_item_price,
    COUNT(*) as items_updated
    FROM menu_items 
    WHERE updated_at >= CURRENT_DATE - INTERVAL '4 weeks'
    GROUP BY 1 ORDER BY 1;
    ```
    - Next step: If you find popular items had 30% higher unavailability this week, what would be your immediate recommendation to the product team?
        - Calculate Revenue Loss 
        - Alert restaurants about their popular out-of-stock items
        - Inventory management tools - help restaurants track/restock popular items
        - Substitute suggestions - when item unavailable, suggest similar items from same/nearby restaurants
    ```sql 
    -- Top items that became unavailable this week vs last week
    WITH popular_items AS (
    SELECT item_id, COUNT(*) as clicks
    FROM user_events 
    WHERE action = 'item_click' 
        AND event_date >= CURRENT_DATE - INTERVAL '2 weeks'
    GROUP BY item_id
    ORDER BY clicks DESC
    LIMIT 50
    ),
    availability_trend AS (
    SELECT 
        pi.item_id,
        pi.clicks,
        DATE_TRUNC('week', ue.event_date) as week,
        SUM(CASE WHEN ue.action = 'item_unavailable_click' THEN 1 ELSE 0 END) as unavailable_clicks,
        SUM(CASE WHEN ue.action = 'item_click' THEN 1 ELSE 0 END) as total_clicks
    FROM popular_items pi
    JOIN user_events ue ON pi.item_id = ue.item_id
    WHERE ue.event_date >= CURRENT_DATE - INTERVAL '2 weeks'
    GROUP BY 1,2,3
    )
    SELECT 
    item_id,
    week,
    unavailable_clicks::DECIMAL / total_clicks as unavailability_rate
    FROM availability_trend
    ORDER BY item_id, week;
    ```
    
    ```sql 
        -- Items with significant price increases
        SELECT 
        item_id,
        restaurant_id,
        LAG(price) OVER (PARTITION BY item_id ORDER BY updated_at) as old_price,
        price as new_price,
        ((price - LAG(price) OVER (PARTITION BY item_id ORDER BY updated_at)) / 
        LAG(price) OVER (PARTITION BY item_id ORDER BY updated_at)) * 100 as price_change_pct
        FROM menu_items
        WHERE updated_at >= CURRENT_DATE - INTERVAL '1 week'
        AND LAG(price) OVER (PARTITION BY item_id ORDER BY updated_at) IS NOT NULL
        HAVING ABS(price_change_pct) > 10  -- Focus on 10%+ changes
        ORDER BY price_change_pct DESC;
    ```
</details>
